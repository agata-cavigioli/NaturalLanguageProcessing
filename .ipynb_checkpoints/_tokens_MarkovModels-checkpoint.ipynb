{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8e1c034-2e6b-482d-8e05-1b281903f7b7",
   "metadata": {},
   "source": [
    "# Ejercicio 1\n",
    "El número de palabras que tiene la novela.\n",
    "El número de palabras únicas que tiene la novela.\n",
    "El número de veces que aparece la palabra \"Macondo\" en la novela.\n",
    "Las 100 palabras más frecuentes de la novela, eliminando las palabras vacías (stopwords)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8eb2a934-5ab3-4ce1-a1ba-5c7d2b151f8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/agata/.local/lib/python3.10/site-packages/torch/cuda/__init__.py:611: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n",
      "2024-01-06 19:02:43.740930: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-01-06 19:02:43.916258: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-01-06 19:02:43.916367: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-01-06 19:02:43.916498: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-01-06 19:02:43.954110: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-01-06 19:02:48.455278: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2024-01-06 19:02:53.755558: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:268] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "/home/agata/.local/lib/python3.10/site-packages/spacy/util.py:910: UserWarning: [W095] Model 'es_core_news_sm' (3.6.0) was trained with spaCy v3.6.0 and may not be 100% compatible with the current version (3.7.2). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of words: 138436\n",
      "\n",
      "number of unique words: 8287\n",
      "\n",
      "number of occurence of the word macondo: 180\n",
      "\n",
      "number of important words: 59717\n",
      "\n",
      "the first 100 most frequent words: \n",
      "{'aureliano': 794, 'úrsula': 514, 'arcadio': 480, 'casa': 463, 'josé': 424, 'buendía': 406, 'coronel': 312, 'amaranta': 310, '«': 282, 'tiempo': 269, 'fernanda': 218, 'años': 187, 'noche': 181, 'macondo': 180, 'guerra': 152, 'cuarto': 141, 'mujer': 140, 'vio': 139, 'hombre': 137, 'muerte': 136, 'mundo': 132, 'volvió': 129, 'rebeca': 129, 'vida': 124, 'padre': 117, 'remedios': 117, 'pueblo': 115, 'hubiera': 115, 'meses': 110, 'puerta': 108, 'hombres': 105, 'melquíades': 103, 'meme': 98, 'dormitorio': 96, 'cama': 94, 'encontró': 93, 'iba': 92, 'cosas': 91, 'puso': 89, 'ojos': 89, 'amor': 87, 'familia': 86, 'niños': 86, 'mano': 85, 'horas': 85, 'único': 84, 'realidad': 83, 'hora': 82, 'madre': 82, 'calle': 80, 'podía': 80, 'corazón': 79, 'parecía': 78, 'frente': 77, 'momento': 76, 'sintió': 74, 'gerineldo': 74, 'llevaba': 73, 'márquez': 71, 'oro': 71, 'tierra': 71, 'crespi': 71, 'hacía': 70, 'petra': 70, 'hijo': 69, 'piedad': 67, 'bella': 67, 'cotes': 67, 'dios': 66, 'pietro': 66, 'nombre': 65, 'patio': 65, 'lugar': 64, 'decía': 64, 'llevó': 63, 'cabeza': 63, 'voz': 62, 'hijos': 62, 'pilar': 62, 'santa': 62, 'lluvia': 60, 'taller': 60, 'siquiera': 59, 'punto': 59, 'niño': 59, 'ternera': 59, 'mañana': 58, 'aire': 58, 'gente': 56, 'mesa': 56, 'marido': 55, 'tiempos': 55, 'compañía': 55, 'ocasión': 53, 'sofía': 53, 'siguió': 52, 'empezó': 52, 'corredor': 52, 'época': 51, 'hermano': 51}\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"es_core_news_sm\")\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "with open('novela.txt','r') as f:\n",
    "    mytext = f.read()\n",
    "    \n",
    "####################################1 \n",
    "txt = mytext.lower()\n",
    "txt = txt.replace(',', '')\n",
    "txt = txt.replace('.', '')\n",
    "txt = txt.replace(';', '')\n",
    "txt = txt.replace(':', '')\n",
    "txt = txt.replace('?', '')\n",
    "txt = txt.replace('¿', '')\n",
    "txt = txt.replace('!', '')\n",
    "txt = txt.replace('¡', '')\n",
    "txt = txt.replace('-', '')\n",
    "txt = txt.replace('»', '')\n",
    "txt = txt.replace('\"', '')\n",
    "txt = txt.replace('\\n', '')\n",
    "\n",
    "doc = nlp(txt)\n",
    "\n",
    "####################################1 \n",
    "\n",
    "i=0\n",
    "for token in doc:\n",
    "    i = i+1\n",
    "print('number of words: ' + str(i) + '\\n')\n",
    "\n",
    "####################################2 El número de palabras únicas que tiene la novela.\n",
    "wordfreq1 = {}\n",
    "for token in doc:\n",
    "    if token.text not in wordfreq1:\n",
    "        wordfreq1[token.text] = 0 \n",
    "    wordfreq1[token.text] += 1\n",
    "\n",
    "j=0\n",
    "for key in wordfreq1:\n",
    "    if wordfreq1[key] == 1:         #unique word\n",
    "        j = j+1\n",
    "print('number of unique words: ' + str(j) + '\\n')\n",
    "\n",
    "\n",
    "##################################3 El número de veces que aparece la palabra \"Macondo\" en la novela.\n",
    "m=0\n",
    "for token in doc:\n",
    "    if token.text == 'macondo':\n",
    "        m = m+1\n",
    "print('number of occurence of the word macondo: ' + str(m) + '\\n')\n",
    "\n",
    "\n",
    "##################################4 Las 100 palabras más frecuentes de la novela, eliminando las palabras vacías (stopwords).\n",
    "k=0\n",
    "imp_text = []\n",
    "for token in doc:\n",
    "    if token.is_stop == False:\n",
    "        k = k+1\n",
    "        imp_text.append(token)\n",
    "print('number of important words: ' + str(k) + '\\n')\n",
    "\n",
    "\n",
    "wordfreq = {}\n",
    "for token in imp_text:\n",
    "    if token.text not in wordfreq:\n",
    "        wordfreq[token.text] = 0 \n",
    "    wordfreq[token.text] += 1\n",
    "\n",
    "wordfreq = sorted(wordfreq.items(), key=lambda x:x[1], reverse = True)\n",
    "\n",
    "wordfreq = dict(wordfreq)\n",
    "\n",
    "frequent = {k: wordfreq[k] for k in list(wordfreq)[:100]}\n",
    "print('the first 100 most frequent words: ')\n",
    "print(frequent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8995403-99c7-4f32-a2bd-4d6fe986c9c3",
   "metadata": {},
   "source": [
    "# Ejercicio 2\n",
    "Crea un modelo del lenguaje basado en unigramas, bigramas y en trigramas para la novela \"Cien años de soledad\" de Gabriel García Márquez. Tokeniza en función de los caracteres, no de las palabras.\n",
    "\n",
    "$$ PPL = 2^{-\\frac{1}{N} \\sum_{i=1}^{N} \\log_2 P(w_i)} $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b8f7163",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "with open('novela.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "#text = text.lower()\n",
    "text = text.replace('\\n', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d32e024",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resolución del ejercio anterior para el modelo de unigramas.\n",
    "\n",
    "# unique character\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "\n",
    "# maps between char and int\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: toma una cadena, devuelve una lista de enteros\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: toma una lista de enteros, devuelve una cadena\n",
    "\n",
    "#print(encode('abc'))\n",
    "#print(decode([1,2,3]))\n",
    "\n",
    "# vector of unigrams\n",
    "unigram_vector = np.zeros((vocab_size))\n",
    "\n",
    "# Recorre el texto y cuenta los bigramas\n",
    "for i in range(len(text)-1):\n",
    "    unigram_vector[stoi[text[i]]] += 1\n",
    "\n",
    "# Normaliza las filas de la tabla\n",
    "unigram_vector = unigram_vector / unigram_vector.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "682700d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'auEshdtae sadlneoba  meclu dñmaaromñim,ibrt rd Aio rmob ímcectf ,oa mn uflerlb níol seeqfebli l oa l'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Genera un texto a partir de la tabla de uniramas\n",
    "\n",
    "import random\n",
    "\n",
    "def generate_text(unigram_vector, n_words, word = ''):\n",
    "    text = [word]\n",
    "    key_vocab = list(range(vocab_size))\n",
    "    for i in range(n_words):\n",
    "        \n",
    "        key = random.choices(key_vocab, unigram_vector)[0]\n",
    "        word = itos[key]\n",
    "        text.append(word)\n",
    "    return ''.join(text)\n",
    "\n",
    "generate_text(unigram_vector, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "89a5ac2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22.417956510708528\n",
      "21.192271502372794\n"
     ]
    }
   ],
   "source": [
    "# Calcula la perplejidad del modelo de lenguaje basado en bigram_table.\n",
    "\n",
    "def perplexity(unigram_vector, text):\n",
    "    perplexity = 0\n",
    "    for i in range(len(text)-1):\n",
    "        c1 = stoi[text[i]]\n",
    "        probability = unigram_vector[c1]\n",
    "        if probability == 0:\n",
    "            probability = 1e-10\n",
    "        perplexity += np.log(probability)\n",
    "    perplexity = np.exp(-perplexity/len(text))\n",
    "    return perplexity\n",
    "\n",
    "\n",
    "txt1= \"Muchos años después, frente al pelotón de fusilamiento, el coronel Aureliano Buendía\"\n",
    "print(perplexity(unigram_vector, txt1))\n",
    "\n",
    "# ¿Qué perplejidad obtienes para el texto invertido?\n",
    "print(perplexity(unigram_vector, txt1[::-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "754ca7af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resolución del ejercio anterior para el modelo de bigramas.\n",
    "\n",
    "# Preparación de la tabla que contendrá los bigramas\n",
    "bigram_table = np.zeros((vocab_size, vocab_size))\n",
    "\n",
    "# Recorre el texto y cuenta los bigramas\n",
    "for i in range(len(text)-1):\n",
    "    bigram_table[stoi[text[i]], stoi[text[i+1]]] += 1\n",
    "\n",
    "# Normaliza las filas de la tabla\n",
    "bigram_table = bigram_table / bigram_table.sum(axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a7fb2122",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Cioba eriveveró Pimás lana l Amás po cel sio cigontendico onerie, ta Mañó- l cato. Tas d y qulanuro c'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Genera un texto a partir de la tabla de bigramas\n",
    "\n",
    "def generate_text(bigram_table, n_words, word=\"H\"):\n",
    "    text = [word]\n",
    "    key_vocab = list(range(vocab_size))\n",
    "    for i in range(n_words):\n",
    "        key = random.choices(key_vocab, bigram_table[stoi[text[-1]]])[0]\n",
    "        word = itos[key]\n",
    "        text.append(word)\n",
    "    return ''.join(text)\n",
    "\n",
    "generate_text(bigram_table, 100, \"C\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cda41181",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.876121540533543\n",
      "857.2947969771436\n"
     ]
    }
   ],
   "source": [
    "# Calcula la perplejidad del modelo de lenguaje basado en bigram_table.\n",
    "\n",
    "def biperplexity(bigram_table, text):\n",
    "    perplexity = 0\n",
    "    for i in range(len(text)-1):\n",
    "        c1 = stoi[text[i]]\n",
    "        c2 = stoi[text[i+1]]\n",
    "        probability = bigram_table[c1][c2]\n",
    "        if probability == 0:\n",
    "            probability = 1e-10\n",
    "        perplexity += np.log(probability)\n",
    "    perplexity = np.exp(-perplexity/len(text))\n",
    "    return perplexity\n",
    "\n",
    "\n",
    "txt1= \"Muchos años después, frente al pelotón de fusilamiento, el coronel Aureliano Buendía\"\n",
    "print(biperplexity(bigram_table, txt1))\n",
    "\n",
    "# ¿Qué perplejidad obtienes para el texto invertido?\n",
    "print(biperplexity(bigram_table, txt1[::-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fde8c921",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resolución del ejercio anterior para el modelo de trigramas.\n",
    "\n",
    "# Preparación de la tabla que contendrá los bigramas\n",
    "trigram_table = np.zeros((vocab_size, vocab_size, vocab_size))\n",
    "\n",
    "# Recorre el texto y cuenta los bigramas\n",
    "for i in range(len(text)-2):\n",
    "    trigram_table[stoi[text[i]], stoi[text[i+1]],stoi[text[i+2]]] += 1\n",
    "\n",
    "# Normaliza las filas de la tabla\n",
    "trigram_table = trigram_table / trigram_table.sum(axis=(1,2), keepdims=True)\n",
    "trigram_table = np.nan_to_num(trigram_table, nan=0.00001)\n",
    "#trigram_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e7a1df34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Macado anoto que Singeda envendo dez. Maunar del desde fue Úrsu de plimeton ausidas, haba mue Aureci'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Genera un texto a partir de la tabla de trigramas\n",
    "\n",
    "def generate_text(trigram_table, n_words, word=\"Ha\"):\n",
    "    text = [word[0],word[1]]\n",
    "    key_vocab = list(range(vocab_size))\n",
    "    for i in range(2,n_words):\n",
    "        key = random.choices(key_vocab, trigram_table[stoi[text[i-2]],stoi[text[i-1]]])[0]\n",
    "        word = itos[key]\n",
    "        text.append(word)\n",
    "    return ''.join(text)\n",
    "\n",
    "generate_text(trigram_table, 100, \"Ma\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ca8fbcab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45.4210762055036\n",
      "423069.51176773186\n"
     ]
    }
   ],
   "source": [
    "def triperplexity(trigram_table, text):\n",
    "    perplexity = 0\n",
    "    for i in range(len(text)-2):\n",
    "        c1 = stoi[text[i]]\n",
    "        c2 = stoi[text[i+1]]\n",
    "        c3 = stoi[text[i+2]]\n",
    "        probability = trigram_table[c1][c2][c3]\n",
    "        if probability == 0:\n",
    "            probability = 1e-10\n",
    "        perplexity += np.log(probability)\n",
    "    perplexity = np.exp(-perplexity/len(text))\n",
    "    return perplexity\n",
    "\n",
    "txt1= \"Muchos años después, frente al pelotón de fusilamiento, el coronel Aureliano Buendía\"\n",
    "print(triperplexity(trigram_table, txt1))\n",
    "\n",
    "# ¿Qué perplejidad obtienes para el texto invertido?\n",
    "print(triperplexity(trigram_table, txt1[::-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efbf9e18",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
