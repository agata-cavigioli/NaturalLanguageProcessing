{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d89cfdc-ec1f-402f-91c8-cae61e12d00e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No se encontraron GPUs disponibles, utilizando la CPU\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Comprobar si hay una GPU disponible\n",
    "if torch.cuda.is_available():\n",
    "    # Seleccionar la GPU por índice (por ejemplo, índice 0 para la primera GPU)\n",
    "    device = torch.device(\"cuda:0\")\n",
    "    print(f\"Usando GPU: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    # Si no hay GPU disponible, utiliza la CPU\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"No se encontraron GPUs disponibles, utilizando la CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d2ef1f68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamaño del vocabulario: 95812 tokens\n",
      "Tokenización de la frase 'Here is an example sentence': ['here', 'is', 'an', 'example', 'sentence']\n",
      "Índices de las palabras 'here', 'is', 'an', 'example', 'supercalifragilisticexpialidocious': [476, 22, 31, 5298, 1]\n",
      "Palabras correspondientes a los índices 475, 21, 30, 5297, 0: ['version', 'at', 'from', 'establish', '<pad>']\n",
      "Las diez primeras palabras del vocabulario: ['<pad>', '<unk>', '.', 'the', ',', 'to', 'a', 'of', 'in', 'and']\n",
      "Tokenización de la frase 'Here is an example sentence': [476, 22, 31, 5298, 2994]\n"
     ]
    }
   ],
   "source": [
    "from torchtext import datasets\n",
    "from torchtext.data import to_map_style_dataset\n",
    "import numpy as np\n",
    "\n",
    "# Load the dataset\n",
    "train_iter, test_iter = datasets.AG_NEWS(split=('train', 'test'))\n",
    "\n",
    "train_ds = to_map_style_dataset(train_iter)\n",
    "test_ds = to_map_style_dataset(test_iter)\n",
    "\n",
    "train = np.array(train_ds)\n",
    "test = np.array(test_ds)\n",
    "\n",
    "# Create vocabulary and embedding\n",
    "\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "\n",
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "\n",
    "vocab = build_vocab_from_iterator(map(lambda x: tokenizer(x[1]), train_iter), specials=['<pad>','<unk>'])\n",
    "vocab.set_default_index(vocab[\"<unk>\"])\n",
    "\n",
    "print(\"Tamaño del vocabulario:\", len(vocab), \"tokens\")\n",
    "print(\"Tokenización de la frase 'Here is an example sentence':\", tokenizer(\"Here is an example sentence\"))\n",
    "print(\"Índices de las palabras 'here', 'is', 'an', 'example', 'supercalifragilisticexpialidocious':\", vocab(['here', 'is', 'an', 'example', 'supercalifragilisticexpialidocious']))\n",
    "print(\"Palabras correspondientes a los índices 475, 21, 30, 5297, 0:\", vocab.lookup_tokens([475, 21, 30, 5297, 0]))\n",
    "print(\"Las diez primeras palabras del vocabulario:\", vocab.get_itos()[:10])\n",
    "\n",
    "text_pipeline = lambda x: vocab(tokenizer(x))\n",
    "label_pipeline = lambda x: int(x) - 1\n",
    "\n",
    "print(\"Tokenización de la frase 'Here is an example sentence':\", text_pipeline(\"Here is an example sentence\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "423e5bf5-8fec-4202-a548-3655856b2631",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text: Wall St. Bears Claw Back Into the Black (Reuters) Reuters - Short-sellers, Wall Street's dwindling\\band of ultra-cynics, are seeing green again.\n",
      "Indexed Sequence: [432, 426, 2, 1606, 14839, 114, 67, 3, 849, 14, 28, 15, 28, 16, 50726, 4, 432, 375, 17, 10, 67508, 7, 52259, 4, 43, 4010, 784, 326, 2]\n"
     ]
    }
   ],
   "source": [
    "# Assuming train_iter is an iterator of tuples (label, text)\n",
    "first_example = next(iter(train_iter))\n",
    "\n",
    "# Using the text_pipeline on the text part of the first example\n",
    "indexed_sequence = text_pipeline(first_example[1])\n",
    "\n",
    "print(\"Original Text:\", first_example[1])\n",
    "print(\"Indexed Sequence:\", indexed_sequence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8d6eedc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/agata/.local/lib/python3.10/site-packages/torch/utils/data/datapipes/iter/combining.py:333: UserWarning: Some child DataPipes are not exhausted when __iter__ is called. We are resetting the buffer and each child DataPipe will read from the start again.\n",
      "  warnings.warn(\"Some child DataPipes are not exhausted when __iter__ is called. We are resetting \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 6069,  4073,    12,    78,    17,    10,  2119,   315,   341,  1247,\n",
      "            17,    84,   337,   583,    19,   131,  1111,     4,   332,  5678,\n",
      "            17,    10,     2,   457,  5704,  5031,    11,   100,   377,   236,\n",
      "             5,    38,  1751,     2,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0],\n",
      "        [  118,    13,    10,  1216,     5,   267,   421,  4507,   280,   933,\n",
      "           174,   118,    29,  1054,    18,   267,   421,  4507, 30276,  2989,\n",
      "           351,     7,     3,   113,   427,     7,  7074,   280,   174,     8,\n",
      "           841,     2,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0],\n",
      "        [  203,  2593,  1951,     5,  5720,  1644,    45,  2165,    45,     8,\n",
      "          1624,     7, 16009,  5746,     4,     3,   288,    99,    18,   920,\n",
      "           432,   375,   361,    22,   432,   375,  3231,     2,    62,  2697,\n",
      "             4,     3,  5425,    21,   203,   123,   424,    79,  1951, 53528,\n",
      "            55,     4,   811,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0],\n",
      "        [   78,  1737,  2119,   413,    12,  1037,   333,     3,   302,     7,\n",
      "         82694, 12939,     9,  5444,  2811,    43,    39,    20,    78,  1037,\n",
      "          1084,   333,     5,   190,  2336,     7,     3,  5891,  2811,  2390,\n",
      "             8,   315,   994,     5,  1847,     5,   158,  1903,   121,    14,\n",
      "          2119,    15,     2,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0]])\n",
      "\n",
      "\n",
      "tensor([3, 0, 2, 3])\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "\n",
    "\n",
    "def collate_batch(batch):\n",
    "    label_list, text_list = [], []\n",
    "    for sample in batch:\n",
    "        label, text = sample\n",
    "        text_list.append(torch.tensor(text_pipeline(text), dtype=torch.long))\n",
    "        label_list.append(label_pipeline(label))\n",
    "    return torch.tensor(label_list, dtype=torch.long), torch.nn.utils.rnn.pad_sequence(text_list, batch_first=True, padding_value=vocab[\"<pad>\"])\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_iter, batch_size=64, shuffle=True, collate_fn=collate_batch\n",
    ")\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "    test_iter, batch_size=64, shuffle=True, collate_fn=collate_batch\n",
    ")\n",
    "\n",
    "for batch in train_dataloader:\n",
    "    print(batch[1][:4])\n",
    "    print(\"\\n\")\n",
    "    print(batch[0][:4])\n",
    "    print(\"\\n\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8c393820",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  209,   294,  1193,  1986,     5,  2186,   353,    30,  1033,  2355,\n",
      "           240,     4,    71,    14,    32,    15,    54,     6, 15919,    21,\n",
      "            71,    13,    10,   185,   523,     8,   240,    85,     6,  1036,\n",
      "          1972,  1193,    29,   299,     5,  4723,     9,  1930,    33,   353,\n",
      "            30,     6,  2355,     8,  1033,     2,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0],\n",
      "        [  710,   286,   333,    43,  2123,  7548,   245,   286,  1785,    43,\n",
      "         13871,    19,    47,    73,   178,   379,   620,  1237,     4,    20,\n",
      "          3174,  6111,     9, 14197,  2955,    60,    24,   255,     5,   216,\n",
      "             3, 38541,  1490,    41,  5466,     2,    25,  8279, 43122,     2,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0],\n",
      "        [ 4331,   321,  9324,   466,     5,   154,  3433,  1995,    37,    29,\n",
      "          1349,     5,   154,     8,    23,   128,   262,   288,   395,    35,\n",
      "          6431,    18,    26,    94,   563,    23,  2692,  4331,   134,     2,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0],\n",
      "        [ 3619, 29605,   296, 22271,  3619,    13,   489,     5,  4875, 13146,\n",
      "           127,   412,   520,    30,   296,    13,    10,  1252,   158,    54,\n",
      "           144,  1415,     4,    19, 17929,     4, 30569,  3375,  2820,    54,\n",
      "            22,  1853,     3, 11453,     7,   296, 10740,     2,     6,  6906,\n",
      "          2384,   182,    25,   594,    36, 10045,    19,     2,     2,     2,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0]])\n",
      "tensor([[-0.0279, -0.0892,  0.1488, -0.0887],\n",
      "        [-0.0279, -0.0892,  0.1488, -0.0887],\n",
      "        [-0.0279, -0.0892,  0.1488, -0.0887],\n",
      "        [-0.0279, -0.0892,  0.1488, -0.0887]], grad_fn=<SliceBackward0>)\n",
      "tensor([0, 3, 2, 3])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class LSTMTextClassificationModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_class):\n",
    "        super(LSTMTextClassificationModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)  # <-- Capa de embedding genérica (no pre-entrenada)\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, num_class)\n",
    "\n",
    "    def forward(self, text):\n",
    "        embedded = self.embedding(text)  # <-- Tras pasar por la capa de embedding, las palabras se representan como vectores\n",
    "        lstm_out, _ = self.lstm(embedded)\n",
    "        # Tomar la última salida de la secuencia LSTM\n",
    "        last_output = lstm_out[:, -1, :]\n",
    "        output = self.fc(last_output)\n",
    "        return output\n",
    "    \n",
    "model = LSTMTextClassificationModel(len(vocab), 32, 64, 4)\n",
    "model.train()\n",
    "\n",
    "for batch in train_dataloader:\n",
    "    predicted_label = model(batch[1])\n",
    "    label = batch[0]\n",
    "    break\n",
    "\n",
    "print(batch[1][:4])\n",
    "print(predicted_label[:4])\n",
    "print(label[:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "022b7b06",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/agata/.local/lib/python3.10/site-packages/torch/utils/data/datapipes/iter/combining.py:333: UserWarning: Some child DataPipes are not exhausted when __iter__ is called. We are resetting the buffer and each child DataPipe will read from the start again.\n",
      "  warnings.warn(\"Some child DataPipes are not exhausted when __iter__ is called. We are resetting \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   500 batches | accuracy    0.254\n",
      "|  1000 batches | accuracy    0.257\n",
      "|  1500 batches | accuracy    0.261\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   1 | time: 201.20s | valid accuracy    0.256 \n",
      "-----------------------------------------------------------\n",
      "|   500 batches | accuracy    0.329\n",
      "|  1000 batches | accuracy    0.414\n",
      "|  1500 batches | accuracy    0.443\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   2 | time: 133.21s | valid accuracy    0.473 \n",
      "-----------------------------------------------------------\n",
      "|   500 batches | accuracy    0.469\n",
      "|  1000 batches | accuracy    0.550\n",
      "|  1500 batches | accuracy    0.705\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   3 | time: 134.20s | valid accuracy    0.754 \n",
      "-----------------------------------------------------------\n",
      "|   500 batches | accuracy    0.774\n",
      "|  1000 batches | accuracy    0.802\n",
      "|  1500 batches | accuracy    0.828\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   4 | time: 135.02s | valid accuracy    0.814 \n",
      "-----------------------------------------------------------\n",
      "|   500 batches | accuracy    0.834\n",
      "|  1000 batches | accuracy    0.850\n",
      "|  1500 batches | accuracy    0.864\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   5 | time: 133.11s | valid accuracy    0.841 \n",
      "-----------------------------------------------------------\n",
      "|   500 batches | accuracy    0.861\n",
      "|  1000 batches | accuracy    0.873\n",
      "|  1500 batches | accuracy    0.886\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   6 | time: 131.96s | valid accuracy    0.862 \n",
      "-----------------------------------------------------------\n",
      "|   500 batches | accuracy    0.881\n",
      "|  1000 batches | accuracy    0.889\n",
      "|  1500 batches | accuracy    0.899\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   7 | time: 132.33s | valid accuracy    0.871 \n",
      "-----------------------------------------------------------\n",
      "|   500 batches | accuracy    0.894\n",
      "|  1000 batches | accuracy    0.900\n",
      "|  1500 batches | accuracy    0.912\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   8 | time: 157.94s | valid accuracy    0.873 \n",
      "-----------------------------------------------------------\n",
      "|   500 batches | accuracy    0.902\n",
      "|  1000 batches | accuracy    0.909\n",
      "|  1500 batches | accuracy    0.918\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   9 | time: 151.65s | valid accuracy    0.873 \n",
      "-----------------------------------------------------------\n",
      "|   500 batches | accuracy    0.911\n",
      "|  1000 batches | accuracy    0.916\n",
      "|  1500 batches | accuracy    0.927\n",
      "-----------------------------------------------------------\n",
      "| end of epoch  10 | time: 148.61s | valid accuracy    0.877 \n",
      "-----------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Hyperparameters\n",
    "EPOCHS = 10  # epoch\n",
    "LR = 5  # learning rate\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=LR)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.1)\n",
    "\n",
    "def train(dataloader):\n",
    "    model.train()\n",
    "    total_acc, total_count, max_acc = 0, 0, 0\n",
    "    log_interval = 500\n",
    "    start_time = time.time()\n",
    "\n",
    "    for idx, (label, text) in enumerate(dataloader):\n",
    "\n",
    "        label, text = label.to(device), text.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        predicted_label = model(text)\n",
    "        loss = criterion(predicted_label, label)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)\n",
    "        optimizer.step()\n",
    "\n",
    "        total_acc += (predicted_label.argmax(1) == label).sum().item()\n",
    "        total_count += label.size(0)\n",
    "        if idx % log_interval == 0 and idx > 0:\n",
    "            elapsed = time.time() - start_time\n",
    "            print('| {:5d} batches '\n",
    "                  '| accuracy {:8.3f}'.format(idx, total_acc / total_count))\n",
    "\n",
    "            if max_acc < total_acc / total_count:\n",
    "                max_acc = total_acc / total_count\n",
    "\n",
    "            total_acc, total_count = 0, 0\n",
    "            start_time = time.time()\n",
    "    return max_acc\n",
    "\n",
    "\n",
    "def evaluate(dataloader):\n",
    "    model.eval()\n",
    "    total_acc, total_count = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for idx, (label, text) in enumerate(dataloader):\n",
    "            predicted_label = model(text)\n",
    "            loss = criterion(predicted_label, label)\n",
    "            total_acc += (predicted_label.argmax(1) == label).sum().item()\n",
    "            total_count += label.size(0)\n",
    "    return total_acc / total_count\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    epoch_start_time = time.time()\n",
    "\n",
    "    accu_train = train(train_dataloader)\n",
    "    accu_val = evaluate(test_dataloader)\n",
    "\n",
    "    #if accu_train > accu_val:\n",
    "    #    scheduler.step()\n",
    "    \n",
    "    print(\"-\" * 59)\n",
    "    print(\n",
    "        \"| end of epoch {:3d} | time: {:5.2f}s | \"\n",
    "        \"valid accuracy {:8.3f} \".format(\n",
    "            epoch, time.time() - epoch_start_time, accu_val\n",
    "        )\n",
    "    )\n",
    "    print(\"-\" * 59)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a49fd29-62ef-4d76-ad54-a7a4b5be9c40",
   "metadata": {},
   "source": [
    "# Practica 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7e716186-df31-4163-ae69-bdd2048fcec9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No se encontraron GPUs disponibles, utilizando la CPU\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Comprobar si hay una GPU disponible\n",
    "if torch.cuda.is_available():\n",
    "    # Seleccionar la GPU por índice (por ejemplo, índice 0 para la primera GPU)\n",
    "    device = torch.device(\"cuda:0\")\n",
    "    print(f\"Usando GPU: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    # Si no hay GPU disponible, utiliza la CPU\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"No se encontraron GPUs disponibles, utilizando la CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9e2f89cb-e941-4b92-b51f-e9e3010d946e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "400000\n"
     ]
    }
   ],
   "source": [
    "from torchtext.vocab import GloVe\n",
    "GLOVEDIM = 100  # 50, 100, 200, or 300\n",
    "glove = GloVe(name='6B', dim=GLOVEDIM) \n",
    "\n",
    "#text_pipeline = lambda x: [glove.get_vecs_by_tokens(token).tolist() for token in tokenizer(x)]\n",
    "label_pipeline = lambda x: int(x) - 1\n",
    "\n",
    "embedding_size = glove.vectors.size(1)\n",
    "print(embedding_size)\n",
    "print(glove.vectors.size(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b278437f-c122-4a2f-8391-195c7c6476eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([69, 100])\n",
      "tensor([[[-0.2314, -0.0691,  1.5072,  ...,  0.7175,  1.7060, -0.1610],\n",
      "         [-0.0720,  0.2313,  0.0237,  ..., -0.7189,  0.8689,  0.1954],\n",
      "         [ 0.3778, -0.1233,  0.7827,  ...,  0.0042,  0.8549, -0.4411],\n",
      "         ...,\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "        [[ 0.7412,  0.1104,  0.5678,  ...,  0.2616,  0.5887,  0.2336],\n",
      "         [ 0.5847, -0.9247, -0.2611,  ..., -0.2471, -0.0023, -0.3864],\n",
      "         [ 0.3590,  0.0196, -0.5449,  ...,  0.1344,  1.0224,  0.1363],\n",
      "         ...,\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "        [[ 0.2490, -0.0982,  0.2848,  ...,  0.5489, -0.7360,  0.2878],\n",
      "         [-0.3015,  0.6174,  0.8359,  ..., -0.3751,  0.5616,  0.4559],\n",
      "         [ 1.3168,  0.7167,  0.3151,  ...,  0.3752,  1.1517,  0.0713],\n",
      "         ...,\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "        [[ 0.4801,  0.1466,  0.7959,  ...,  0.3337,  1.0150,  0.0887],\n",
      "         [ 0.2457,  0.3463,  0.6535,  ...,  0.4007,  0.8481, -0.5680],\n",
      "         [-0.3272,  0.0964,  0.3424,  ..., -0.3817,  0.4399,  0.2468],\n",
      "         ...,\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]]])\n",
      "\n",
      "\n",
      "tensor([3, 3, 1, 0])\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def collate_batch(batch):\n",
    "    label_list, text_list = [], []\n",
    "    max_len = max([len(tokenizer(sample[1])) for sample in batch])\n",
    "    for sample in batch:\n",
    "        label, text = sample\n",
    "        \n",
    "        embed_list = glove.get_vecs_by_tokens(tokenizer(text))\n",
    "        padding_list = glove[0].unsqueeze(0).repeat(max_len - len(embed_list), 1) \n",
    "                       #2d padding vector repeated\n",
    "        embed_list = torch.cat((embed_list, padding_list), 0) \n",
    "        #text_list.append(torch.tensor(text_pipeline(text))) tensor of token\n",
    "        text_list.append(embed_list)\n",
    "        label_list.append(label_pipeline(label))\n",
    "    #return torch.tensor(label_list, dtype=torch.long), torch.nn.utils.rnn.pad_sequence(text_list, batch_first=True, padding_value=vocab[\"<pad>\"])\n",
    "    return torch.tensor(label_list, dtype=torch.long).to(device), torch.stack(text_list).to(device)\n",
    "    \n",
    "train_dataloader = DataLoader(train_iter, batch_size=64, shuffle=True, collate_fn=collate_batch)\n",
    "test_dataloader = DataLoader(test_iter, batch_size=64, shuffle=True, collate_fn=collate_batch)\n",
    "\n",
    "for batch in train_dataloader:\n",
    "    print(batch[1][1].shape)\n",
    "    print(batch[1][:4])\n",
    "    print(\"\\n\")\n",
    "    print(batch[0][:4])\n",
    "    print(\"\\n\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e83dd334-1c79-4711-95de-7511281ca4c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class LSTMTextClassificationModelGloVe(nn.Module):\n",
    "    def __init__(self, embed_dim, hidden_dim, num_class):\n",
    "        super(LSTMTextClassificationModelGloVe, self).__init__()\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, num_class)\n",
    "\n",
    "    def forward(self, embedding):\n",
    "        lstm_out, _ = self.lstm(embedding)\n",
    "        last_output = lstm_out[:, -1, :]\n",
    "        output = self.fc(last_output)\n",
    "        return output\n",
    "        \n",
    "model = LSTMTextClassificationModelGloVe(GLOVEDIM, 64, 4).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "081501af-9a68-461e-9a82-d0821e804ea1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   500 batches | accuracy    0.250\n",
      "|  1000 batches | accuracy    0.251\n",
      "|  1500 batches | accuracy    0.267\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   1 | time: 256.19s | valid accuracy    0.672 \n",
      "-----------------------------------------------------------\n",
      "|   500 batches | accuracy    0.684\n",
      "|  1000 batches | accuracy    0.808\n",
      "|  1500 batches | accuracy    0.876\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   2 | time: 211.75s | valid accuracy    0.878 \n",
      "-----------------------------------------------------------\n",
      "|   500 batches | accuracy    0.878\n",
      "|  1000 batches | accuracy    0.888\n",
      "|  1500 batches | accuracy    0.905\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   3 | time: 210.26s | valid accuracy    0.892 \n",
      "-----------------------------------------------------------\n",
      "|   500 batches | accuracy    0.892\n",
      "|  1000 batches | accuracy    0.901\n",
      "|  1500 batches | accuracy    0.919\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   4 | time: 211.03s | valid accuracy    0.906 \n",
      "-----------------------------------------------------------\n",
      "|   500 batches | accuracy    0.897\n",
      "|  1000 batches | accuracy    0.903\n",
      "|  1500 batches | accuracy    0.923\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   5 | time: 210.58s | valid accuracy    0.908 \n",
      "-----------------------------------------------------------\n",
      "|   500 batches | accuracy    0.893\n",
      "|  1000 batches | accuracy    0.905\n",
      "|  1500 batches | accuracy    0.922\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   6 | time: 210.43s | valid accuracy    0.908 \n",
      "-----------------------------------------------------------\n",
      "|   500 batches | accuracy    0.895\n",
      "|  1000 batches | accuracy    0.904\n",
      "|  1500 batches | accuracy    0.923\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   7 | time: 209.19s | valid accuracy    0.907 \n",
      "-----------------------------------------------------------\n",
      "|   500 batches | accuracy    0.897\n",
      "|  1000 batches | accuracy    0.903\n",
      "|  1500 batches | accuracy    0.924\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   8 | time: 210.92s | valid accuracy    0.908 \n",
      "-----------------------------------------------------------\n",
      "|   500 batches | accuracy    0.896\n",
      "|  1000 batches | accuracy    0.904\n",
      "|  1500 batches | accuracy    0.924\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   9 | time: 210.80s | valid accuracy    0.907 \n",
      "-----------------------------------------------------------\n",
      "|   500 batches | accuracy    0.897\n",
      "|  1000 batches | accuracy    0.904\n",
      "|  1500 batches | accuracy    0.922\n",
      "-----------------------------------------------------------\n",
      "| end of epoch  10 | time: 211.43s | valid accuracy    0.907 \n",
      "-----------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "EPOCHS = 10  # epoch\n",
    "LR = 5  # learning rate\n",
    "BATCH_SIZE = 8  # batch size for training\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=LR)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.1)\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    epoch_start_time = time.time()\n",
    "\n",
    "    accu_train = train(train_dataloader)\n",
    "    accu_val = evaluate(test_dataloader)\n",
    "\n",
    "    if accu_train > accu_val:\n",
    "        scheduler.step()\n",
    "    \n",
    "    print(\"-\" * 59)\n",
    "    print(\n",
    "        \"| end of epoch {:3d} | time: {:5.2f}s | \"\n",
    "        \"valid accuracy {:8.3f} \".format(\n",
    "            epoch, time.time() - epoch_start_time, accu_val\n",
    "        )\n",
    "    )\n",
    "    print(\"-\" * 59)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d3a82e-5856-4e34-8c31-de66b564792c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
