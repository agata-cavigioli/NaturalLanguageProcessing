{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f6dfb05-5d3f-4333-bc9b-2beeaa94c1c2",
   "metadata": {},
   "source": [
    "# ATENCION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "16112264-9927-42fa-b2ac-9429fa9e12a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 3])\n",
      "torch.Size([4, 3])\n",
      "torch.Size([4, 3])\n",
      "torch.Size([4, 4])\n",
      "tensor([[0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1732, 0.3464, 0.5196, 0.6928],\n",
      "        [0.0346, 0.0693, 0.1039, 0.1386],\n",
      "        [0.0520, 0.1039, 0.1559, 0.2078]])\n",
      "tensor([[0.2500, 0.2500, 0.2500, 0.2500],\n",
      "        [0.1892, 0.2250, 0.2676, 0.3182],\n",
      "        [0.2372, 0.2455, 0.2542, 0.2631],\n",
      "        [0.2309, 0.2432, 0.2561, 0.2698]])\n",
      "\n",
      " tensor([[0.2500, 0.5000, 0.5000],\n",
      "        [0.1892, 0.5432, 0.5857],\n",
      "        [0.2372, 0.5087, 0.5173],\n",
      "        [0.2309, 0.5130, 0.5260]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "\n",
    "Q = torch.tensor([[0.0, 0.0, 0.0], [1, 1, 1], [0.2, 0.2, 0.2], [0.3, 0.3, 0.3]])\n",
    "K = torch.tensor([[0.1, 0.1, 0.1], [0.2, 0.2, 0.2], [0.3, 0.3, 0.3], [0.4, 0.4, 0.4]])\n",
    "V = torch.tensor([[1., 0., 0.], [0., 1., 0.], [0., 0., 1.], [0., 1., 1.]])\n",
    "\n",
    "S =torch.mm(Q,K.T)\n",
    "S=S/np.sqrt(K.shape[1])\n",
    "\n",
    "n = S.shape[0]\n",
    "print(S)\n",
    "\n",
    "S = F.softmax(S, dim = 1)\n",
    "print(S)\n",
    "A = torch.sum(S[:, :, None] * V[None, :, :], dim=1)\n",
    "\n",
    "print('\\n',A)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfaa2c27-c022-49d3-9863-3420504d7b18",
   "metadata": {},
   "source": [
    "# ATENCION CON ENMASCARAMENTO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b1388456-2d38-4019-8c2e-ccb3bc001314",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0000,   -inf,   -inf,   -inf],\n",
      "        [0.1732, 0.3464,   -inf,   -inf],\n",
      "        [0.0346, 0.0693, 0.1039,   -inf],\n",
      "        [0.0520, 0.1039, 0.1559, 0.2078]])\n",
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.4568, 0.5432, 0.0000, 0.0000],\n",
      "        [0.3219, 0.3332, 0.3449, 0.0000],\n",
      "        [0.2309, 0.2432, 0.2561, 0.2698]])\n",
      "\n",
      " tensor([[1.0000, 0.0000, 0.0000],\n",
      "        [0.4568, 0.5432, 0.0000],\n",
      "        [0.3219, 0.3332, 0.3449],\n",
      "        [0.2309, 0.5130, 0.5260]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "Q = torch.tensor([[0.0, 0.0, 0.0], [1, 1, 1], [0.2, 0.2, 0.2], [0.3, 0.3, 0.3]])\n",
    "K = torch.tensor([[0.1, 0.1, 0.1], [0.2, 0.2, 0.2], [0.3, 0.3, 0.3], [0.4, 0.4, 0.4]])\n",
    "V = torch.tensor([[1., 0., 0.], [0., 1., 0.], [0., 0., 1.], [0., 1., 1.]])\n",
    "\n",
    "S =torch.mm(Q,K.T)\n",
    "S=S/np.sqrt(K.shape[1])\n",
    "\n",
    "n = S.shape[0]\n",
    "mask = torch.tril(torch.ones(n,n))==1  #triangular matrix\n",
    "S = torch.where(mask, S, float('-inf'))\n",
    "print(S)\n",
    "\n",
    "S = F.softmax(S, dim = 1)\n",
    "print(S)\n",
    "A = torch.sum(S[:, :, None] * V[None, :, :], dim=1)\n",
    "\n",
    "print('\\n',A)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47041aeb-af2a-424e-898e-d3386f1dff05",
   "metadata": {},
   "source": [
    "# EINSTEIN SUM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "28697933-b623-4abd-93c8-992ffc75360b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(torch.Size([3, 4, 5]), tensor([[[-1.7706e-02,  2.8828e+00, -7.1356e+00,  4.5344e+00, -1.5819e-01],\n",
      "         [-4.4755e-02,  1.4575e+00, -4.1217e-01, -1.8539e+00, -2.6193e+00],\n",
      "         [-1.7705e-02,  2.8828e+00, -7.1356e+00,  4.5344e+00, -1.5820e-01],\n",
      "         [ 4.7722e-01,  1.0455e+00,  1.3967e+00, -4.7866e-01, -2.3153e+00]],\n",
      "\n",
      "        [[-3.3266e-03,  5.6681e+00, -3.7679e+00,  2.1016e+00, -2.5847e+00],\n",
      "         [ 4.2982e-01,  2.9642e+00, -1.3427e+00,  9.1846e-01, -2.3861e-01],\n",
      "         [ 1.7429e+00,  2.8946e+00,  8.1312e-01,  3.0068e+00, -2.4894e-01],\n",
      "         [ 1.1529e+00,  2.9465e+00, -2.5406e-01,  2.0860e+00, -4.9052e-01]],\n",
      "\n",
      "        [[-5.6236e-01,  1.4978e-01,  2.3013e+00,  1.3614e+00,  4.1825e+00],\n",
      "         [ 2.5039e-01, -1.1190e+00,  9.3869e-01, -1.5250e+00, -2.4084e+00],\n",
      "         [ 1.8348e+00,  3.9938e-01, -3.3142e-01,  3.7015e+00, -4.4636e+00],\n",
      "         [ 1.2122e+00, -1.6552e-01,  6.8812e-01,  1.3289e+00, -3.5608e+00]]]))\n",
      "torch.Size([3, 4, 5])\n",
      "torch.Size([5, 4, 3])\n",
      "torch.Size([5, 4, 1])\n",
      "torch.Size([1, 4, 5])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (20x4 and 15x1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 50\u001b[0m\n\u001b[1;32m     47\u001b[0m     context_vector \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39meinsum(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msnk,snl->knl\u001b[39m\u001b[38;5;124m\"\u001b[39m, attention, encoder_states)\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28mprint\u001b[39m(attention(x, encoder_states, hidden, cell))\n\u001b[0;32m---> 50\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mEinsteinSum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoder_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcell\u001b[49m\u001b[43m)\u001b[49m)\n",
      "Cell \u001b[0;32mIn[19], line 39\u001b[0m, in \u001b[0;36mEinsteinSum\u001b[0;34m(x, encoder_states, hidden, cell)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28mprint\u001b[39m(cell\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     38\u001b[0m energyfun \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLinear(hidden_size \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 39\u001b[0m energy \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mReLU(\u001b[43menergyfun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoder_states\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mT\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     41\u001b[0m attention \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mSoftmax(energy)\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# attention: (seq_length, N, 1)\u001b[39;00m\n\u001b[1;32m     43\u001b[0m \n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# attention: (seq_length, N, 1), snk\u001b[39;00m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;66;03m# encoder_states: (seq_length, N, hidden_size*2), snl\u001b[39;00m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m# we want context_vector: (1, N, hidden_size*2), i.e knl\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (20x4 and 15x1)"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "\n",
    "batch_size = 3\n",
    "sequence_size = 4\n",
    "embedding_size = 5\n",
    "hidden_size = 5\n",
    "\n",
    "x = torch.randn(batch_size, sequence_size,embedding_size)\n",
    "encoder_states = torch.randn(batch_size, sequence_size,embedding_size)\n",
    "hidden = torch.randn(1, sequence_size,embedding_size)\n",
    "cell = torch.randn(1, sequence_size,embedding_size)\n",
    "\n",
    "def attention(x, encoder_states, hidden, cell):\n",
    "    #sequence_length = encoder_states.shape[0]\n",
    "\n",
    "    Q = torch.randn(hidden_size, hidden_size)\n",
    "    K = torch.randn(hidden_size, hidden_size)\n",
    "    V = torch.randn(hidden_size, embedding_size)\n",
    "\n",
    "    Q = torch.matmul(hidden, Q)\n",
    "    K = torch.matmul(encoder_states, K)\n",
    "    V = torch.matmul(encoder_states, V)\n",
    "        \n",
    "    S = torch.matmul(Q, K.transpose(-2, -1))  # Transpose K along the last two dimensions\n",
    "    S = S / np.sqrt(K.shape[-1])\n",
    "\n",
    "    S = F.softmax(S, dim=-1)\n",
    "    context_vector = torch.sum(S.unsqueeze(-1) * V.unsqueeze(1), dim=-2)\n",
    "    return context_vector.shape, context_vector\n",
    "\n",
    "\n",
    "def EinsteinSum(x, encoder_states, hidden, cell):\n",
    "    print(x.shape)\n",
    "    print(encoder_states.T.shape)\n",
    "    print(hidden.T.shape)\n",
    "    print(cell.shape)\n",
    "\n",
    "    energyfun = nn.Linear(hidden_size * 3, 1)\n",
    "    energy = nn.ReLU(energyfun(torch.cat((hidden, encoder_states), dim=2)))\n",
    "\n",
    "    attention = nn.Softmax(energy)\n",
    "    # attention: (seq_length, N, 1)\n",
    "\n",
    "    # attention: (seq_length, N, 1), snk\n",
    "    # encoder_states: (seq_length, N, hidden_size*2), snl\n",
    "    # we want context_vector: (1, N, hidden_size*2), i.e knl\n",
    "    context_vector = torch.einsum(\"snk,snl->knl\", attention, encoder_states)\n",
    "\n",
    "print(attention(x, encoder_states, hidden, cell))\n",
    "print(EinsteinSum(x, encoder_states, hidden, cell))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "628c20fd-10a5-477e-a1d9-0e0d27c8bd52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 5, 256])\n",
      "torch.Size([512, 5, 10])\n",
      "torch.Size([768, 1, 5])\n",
      "torch.Size([5, 1, 256])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Sizes of tensors must match except in dimension 2. Expected size 5 but got size 10 for tensor number 1 in the list.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 34\u001b[0m\n\u001b[1;32m     31\u001b[0m hidden \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m1\u001b[39m, hidden_size \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m3\u001b[39m)  \u001b[38;5;66;03m# N=5, hidden_size*3=768\u001b[39;00m\n\u001b[1;32m     32\u001b[0m cell \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m1\u001b[39m, hidden_size)  \u001b[38;5;66;03m# N=5, hidden_size=256\u001b[39;00m\n\u001b[0;32m---> 34\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mEinsteinSum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoder_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcell\u001b[49m\u001b[43m)\u001b[49m)\n",
      "Cell \u001b[0;32mIn[20], line 12\u001b[0m, in \u001b[0;36mEinsteinSum\u001b[0;34m(x, encoder_states, hidden, cell)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(cell\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     11\u001b[0m energyfun \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLinear(hidden_size \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 12\u001b[0m energy \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(energyfun(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoder_states\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m))\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Apply Softmax along the last dimension (dim=2)\u001b[39;00m\n\u001b[1;32m     15\u001b[0m attention \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39msoftmax(energy, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Sizes of tensors must match except in dimension 2. Expected size 5 but got size 10 for tensor number 1 in the list."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def EinsteinSum(x, encoder_states, hidden, cell):\n",
    "    print(x.shape)\n",
    "    print(encoder_states.T.shape)\n",
    "    print(hidden.T.shape)\n",
    "    print(cell.shape)\n",
    "\n",
    "    energyfun = nn.Linear(hidden_size * 3, 1)\n",
    "    energy = F.relu(energyfun(torch.cat((hidden, encoder_states), dim=2)))\n",
    "\n",
    "    # Apply Softmax along the last dimension (dim=2)\n",
    "    attention = F.softmax(energy, dim=2)\n",
    "    # attention: (seq_length, N, 1)\n",
    "\n",
    "    # attention: (seq_length, N, 1), snk\n",
    "    # encoder_states: (seq_length, N, hidden_size*2), snl\n",
    "    # we want context_vector: (1, N, hidden_size*2), i.e knl\n",
    "    context_vector = torch.einsum(\"snk,snl->knl\", attention, encoder_states)\n",
    "    return context_vector\n",
    "\n",
    "# Assuming hidden_size is defined somewhere in your code\n",
    "hidden_size = 256\n",
    "\n",
    "# Assuming you have defined x, encoder_states, hidden, cell\n",
    "# For illustration purposes, you can replace these with actual tensors in your implementation.\n",
    "x = torch.randn(10, 5, hidden_size)  # seq_length=10, batch_size=5, hidden_size=256\n",
    "encoder_states = torch.randn(10, 5, hidden_size * 2)  # seq_length=10, batch_size=5, hidden_size*2=512\n",
    "hidden = torch.randn(5, 1, hidden_size * 3)  # N=5, hidden_size*3=768\n",
    "cell = torch.randn(5, 1, hidden_size)  # N=5, hidden_size=256\n",
    "\n",
    "print(EinsteinSum(x, encoder_states, hidden, cell))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55617592-aeab-4e66-adf1-19b4133822b4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
