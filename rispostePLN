1 - ¿En qué consiste el proceso de tokenización?

Tokenization is the process of breaking down a piece of text into smaller units called tokens. These tokens can be words, characters, or subwords. The primary goal is to create a structured and standardized representation of the text that can be easily processed by machines. It involves several steps: first, the text is segmented into individual units, like words or subwords. Then, these units are assigned unique identifiers, known as tokens. Punctuation and special characters might also be treated as separate tokens. This process helps standardize input data, reducing complexity and making it easier for algorithms to comprehend and process textual information. Tokenization plays a pivotal role in transforming unstructured text into structured data, facilitating various language-based computational tasks.

2 - ¿Cuál es la diferencia entre lematización y derivación o stemming?

Lemmatization and stemming are both techniques used in natural language processing (NLP) to reduce words to their base or root forms, but they differ in their approaches. Stemming involves cutting off prefixes or suffixes of words to extract a common base form, called the "stem." However, stems might not always be actual words. For instance, "running" might be reduced to "run," but "happier" might become "happi." Lemmatization, on the other hand, aims for the canonical or dictionary form of a word, known as the "lemma." It considers the context and part of speech of the word to determine the root. So, "is," "am," and "are" all get reduced to "be" because they are different forms of the same verb. In summary, stemming is simpler and faster but may generate stems that aren't actual words, while lemmatization produces valid words but is more complex due to its reliance on linguistic rules and a dictionary.

3 - ¿A qué denominamos part-of-speech (POS)?

Part-of-speech (POS) represents the grammatical function of each word within a sentence. It's a linguistic tool that categorizes words based on their roles and relationships in a sentence structure. Understanding the POS of a word—whether it's a noun, verb, adjective, adverb, pronoun, preposition, conjunction, or interjection—helps decipher its context, function, and syntactic connections within a sentence. POS tagging involves sophisticated algorithms that analyze linguistic features, context, and word morphology to accurately assign these grammatical labels. By tagging words with their respective POS, natural language processing systems gain insights into sentence structures, enabling tasks like parsing, sentiment analysis, machine translation, and information extraction. Accurate POS tagging enhances the precision of language models and aids in more nuanced and contextually appropriate language understanding and generation.

4 - ¿Cómo funciona el método de tokenización byte-pair-encoding?

Byte Pair Encoding (BPE) is a method utilized in natural language processing and compression. It operates by iteratively merging the most common pair of consecutive bytes or characters to form a vocabulary of tokens. Initially, the process starts by setting up the vocabulary with individual characters or bytes found in the corpus. Subsequently, the corpus undergoes tokenization where frequent pairs of consecutive tokens are merged to create a single token, and the vocabulary is updated accordingly. The algorithm continues by identifying the most frequent token pairs in the corpus and merging them into a single token. This iterative merging process persists for a defined number of iterations or until a predetermined vocabulary size is reached. Ultimately, the resulting vocabulary comprises tokens representing common character sequences or byte pairs. These tokens can denote words, subwords, or even individual characters, depending on the desired level of granularity. In practical application, when encoding new text, BPE replaces frequent byte pairs or characters with the corresponding tokens from the generated vocabulary. This approach accommodates both common and infrequent terms by representing words as combinations of smaller meaningful units, proving beneficial in tasks like machine translation, language modeling, and managing languages with intricate word structures.

5 - ¿Qué entendemos por modelo del lenguaje?


A language model is a core component of natural language processing (NLP) that enables computers to understand, generate, and predict human language. It's a statistical or machine learning-based system trained on vast amounts of text data to learn word probabilities and sequences of appearing in a context, within a language. These models come in statistical and neural forms, using techniques like recurrent neural networks (RNNs) or transformer-based architectures. They learn the likelihood of word sequences based on context, helping tasks like translation, text generation, and sentiment analysis. In essence, language models decode and generate language, aiding in predicting next words in a sentence, understanding context, and generating coherent text. They are vital for various AI applications reliant on language comprehension and generation.

6 - ¿Qué es un corpus?

A corpus refers to a large and structured collection of text or spoken language data that's systematically gathered and stored for linguistic analysis, language model training, or research purposes. It comprises diverse sources like books, articles, transcripts, speeches, social media posts, and more, assembled to represent a specific language, domain, or variety of languages. Corpora (plural of corpus) serve as essential resources for studying language patterns, understanding linguistic structures, training language models, and conducting research in fields like linguistics, natural language processing (NLP), and machine learning. These collections are annotated or tagged with linguistic information like part-of-speech tags, syntactic structures, or semantic annotations, enhancing their utility in various language-related studies and computational tasks. Overall, corpora play a pivotal role in analyzing, understanding, and modeling natural language.

7 - ¿Qué se pretende medir con la perplejidad? Explica su funcionamiento (no vale solo
la fórmula)

Perplexity is a measurement used to evaluate the performance of language models by assessing how well they predict a sequence of words. It quantifies how surprised or uncertain a model is when predicting the next word in a sequence given the context of preceding words.Essentially, the lower the perplexity score, the better the language model's performance. A lower perplexity indicates that the model is more confident and accurate in predicting the next word in a sequence. The functioning of perplexity involves using a language model to estimate the probability of a sequence of words. The model calculates the probability of each word in a sequence given the context of the preceding words. Perplexity is then computed as the inverse probability of the entire sequence, normalized by the number of words:

Perplexity = 2^{-\frac{1}{N} \sum_{i=1}^{N} \log_{2} P(w_i | w_{i-1}, w_{i-2}, ... w_{1})}

Here, P(w_i | w_{i-1}, w_{i-2}, ... w_{1}) represents the probability of word w_i given the context of the preceding words in the sequence. Lower perplexity scores indicate that the language model is more effective at p redicting the sequence of words. It suggests that the model is better at capturing the underlying patterns and structure of the language, resulting in more accurate predictions and lower uncertainty when generating or understanding text.

8 - Enumera las ventajas y desventajas del modelo bag-of-words (BoW)? 

BoW's strengths lie in its simplicity and versatility. It's straightforward to grasp and implement, making it an accessible tool in NLP tasks. Its applicability spans various tasks like sentiment analysis, text classification, and information retrieval. BoW also handles tasks where the sequence of words is unimportant, proving effective in scenarios where word order doesn't impact the analysis. Additionally, with large datasets, BoW can efficiently reduce the dimensionality of the data, aiding in machine learning models' efficiency. However, BoW comes with its limitations. One major drawback is its disregard for word sequence, leading to a loss of crucial information about sentence structure and meaning. It lacks an understanding of context, which can be crucial for interpreting polysemous words or those whose meanings change based on context. Moreover, with extensive vocabularies, BoW representations tend to become highly sparse, requiring significant storage and computational resources. It also faces challenges in handling unknown words not present in its trained vocabulary. When using the Bag-of-Words model, balancing its simplicity and versatility against its limitations is essential, particularly in applications where context and word order are critical for accurate language understanding and processing.

9 - similitud del coseno, por ejemplo entre BoW

The cosine similarity is a measure used to assess the similarity between two vectors in a multi-dimensional space, commonly applied in text analysis and data mining. In the context of natural language processing (NLP), it's used to compare the similarity between two documents represented as term vectors. This measure aims to calculate the angle between two term vectors in a vector space. The closer the angle between the vectors, the higher their cosine similarity. A smaller angle (close to 0 degrees) indicates higher similarity, meaning the documents share similar terms in the same proportion.
The formula to calculate cosine similarity between two vectors AA and BB is represented as: \text{Cosine Similarity} = \frac{A \cdot B}{\|A\| \times \|B\|}
In summary, cosine similarity measures how similar two documents or term vectors are in a vector space, considering direction rather than magnitude. This metric is widely used in information retrieval, document retrieval, clustering, and recommendation systems to determine similarity between documents, keywords, or any other set of features represented as vectors.

10 - Describe el algoritmo TF-IDF.

The TF-IDF (Term Frequency-Inverse Document Frequency) algorithm is a technique used to assess the importance of a word in a document relative to a collection of documents. It's commonly employed in information retrieval and text mining to weight words based on their relevance in a corpus.
The TF-IDF calculation occurs in two parts:
Term Frequency (TF): It measures how frequently a word appears in a specific document compared to the total number of words in that document. It's calculated as:Total number of terms in document d/Number of times term t appears in document d. Inverse Document Frequency (IDF): It evaluates the significance of a term across the entire set of documents. It's calculated as:log( Total number of documents in the corpus D/ Number of documents containing term t+1)
The TF-IDF score for a term in a specific document is obtained by multiplying the TF by the IDF: The resulting TF-IDF value quantifies the importance of a term in a particular document relative to the entire corpus. Words with higher TF-IDF scores are considered more relevant to that document and the overall document collection. This technique helps emphasize important terms while reducing the importance of common words appearing in many documents. It's used in tasks such as information retrieval, text classification, and text mining to identify key and relevant terms within a corpus.

11 - ¿A qué se debe el nombre “Naive” en el método de clasificación Naive Bayes?

The term "Naive" in the Naive Bayes classification method refers to a simplified yet effective assumption made by this algorithm. This assumption is that all the features used for classification are independent of each other, meaning they have no relationship among themselves. It's considered "naive" because in the real world, it's common for features in a dataset to be correlated or interrelated in some way. However, the Naive Bayes classifier assumes this independence among features to simplify calculations and ease the classification process. Despite this simplified assumption, the Naive Bayes classifier often produces surprisingly good results across a wide range of classification problems. It's particularly effective when working with moderate to large-sized datasets and when the approximate independence between features is reasonably valid.

12 - ¿Qué es word2vec?

Word2Vec is a popular algorithm used in natural language processing (NLP) to generate word embeddings, which are numerical representations of words in a high-dimensional space. These word embeddings capture semantic relationships between words based on their contextual usage in a large corpus of text. The algorithm is based on a neural network model that learns to map words to vectors of real numbers. There are two primary architectures used in Word2Vec: Continuous Bag of Words (CBOW) and Skip-gram. 
CBOW: This model predicts the current word based on its context words within a window. It learns to predict a target word given the surrounding words.
Skip-gram: In contrast, Skip-gram predicts the surrounding context words given a target word. It tries to predict the context words based on a given target word.
Word2Vec operates by learning vector representations for words where words with similar meanings are positioned closer together in the vector space. These learned embeddings often capture semantic relationships; for example, words like "king" and "queen" might be closer together in this vector space, signifying their semantic similarity. These word embeddings obtained through Word2Vec are valuable in various NLP tasks such as language translation, sentiment analysis, recommendation systems, and more. They facilitate the understanding of semantic similarities and relationships between words, contributing significantly to the performance of many language-based machine learning models.

13 - ¿Cuál es la diferencia entre skip-gram y CBOW?

The main difference between Skip-gram and Continuous Bag of Words (CBOW) lies in how they train and predict words in the Word2Vec model.
CBOW (Continuous Bag of Words): CBOW predicts the target word based on the context words within a window. It takes a set of context words as input to predict the target word. The architecture aggregates the context words' vectors to predict the target word. CBOW is faster to train and tends to work well with frequent words.
Skip-gram: Skip-gram predicts surrounding context words given a target word.
It uses a target word to predict the context words within a certain window.
The model handles each context-target pair individually, generating multiple training examples from one input word. Skip-gram is slower to train but can capture rare words and phrases better due to its training mechanism.
In essence, CBOW predicts a target word from its context, while Skip-gram predicts the context words from a target word. The choice between these architectures often depends on the dataset size, the nature of the text corpus, and the specific NLP task at hand.

14 - Describe el algoritmo GloVE.

The GloVe (Global Vectors for Word Representation) algorithm is a technique for generating word embeddings, representing words as numerical vectors. It operates by constructing a word co-occurrence matrix from a large text corpus, indicating how often words appear together within a context. GloVe's objective is to learn these word embeddings such that the dot product of vectors equals the logarithm of the words' co-occurrence probabilities. By optimizing embeddings to minimize the difference between actual and predicted co-occurrence counts, GloVe captures semantic relationships between words. Unlike some methods that rely solely on local context or global statistics, GloVe combines both, utilizing word-word co-occurrences and local context information. This integration enables GloVe embeddings to effectively represent words' semantic meanings and relationships.
The algorithm's innovation lies in its ability to create embeddings that encapsulate rich semantic information derived from how words co-occur in a corpus. These embeddings are widely used across natural language processing tasks like language translation, sentiment analysis, and information retrieval.

15 - ¿Qué desventajas tienen las RNN?

Vanishing or Exploding Gradients: RNNs often struggle with vanishing or exploding gradients during training. When sequences are long, the gradients can become extremely small (vanishing gradients) or extremely large (exploding gradients), making it challenging for the network to learn dependencies or propagate useful information over long sequences.
Difficulty in Capturing Long-Term Dependencies: Although RNNs can theoretically retain information from earlier inputs, they can have difficulty effectively capturing long-range dependencies. This limitation impacts their ability to remember context or information from earlier time steps in very long sequences.
Training Time and Computational Cost: RNNs can be computationally expensive to train, especially when dealing with large datasets and complex architectures like those with many recurrent layers. This complexity results in longer training times and increased computational requirements.
Lack of Parallelism: The sequential nature of RNNs restricts parallel computation since each time step depends on the previous one. This limitation hampers their efficiency, especially in scenarios where parallel processing could significantly speed up computations.

16 - ¿Qué problema solucionan las LSTM? memoria pero no recurrencia

Vanishing and Exploding Gradients: LSTMs mitigate the vanishing and exploding gradient problems that affect the training of RNNs. They achieve this by using a gated structure that regulates the flow of information, enabling better gradient flow through the network during training.
Capturing Long-Term Dependencies: LSTMs excel at capturing and retaining information over long sequences. Through their architecture, which includes a memory cell and gating mechanisms, LSTMs are designed to selectively remember or forget information over extended periods, enabling them to handle and learn from dependencies spread across time steps more effectively.
In summary, LSTMs specifically address the challenges faced by standard RNNs in handling long-range dependencies and learning from sequences with substantial temporal gaps. They achieve this by incorporating memory cells and gated mechanisms that regulate information flow, facilitating better learning and retention of information over extended sequences.

17 - ¿Cuál es la diferencia entre el “hidden state” y la “puerta de salida” en la LSTM?

The "hidden state" and the "output gate" are integral components within a Long Short-Term Memory (LSTM) cell:
Hidden State: The hidden state represents the internal information within an LSTM cell at a specific time step. It serves as the primary output of an LSTM cell and contains information about the current context and selected long-term memories that are chosen to persist.
Output Gate: Within an LSTM, the output gate controls the information that will be emitted as the output from the hidden state. It determines which portion of the hidden state will be used as the output of the LSTM cell at a given time step.
In essence, the hidden state encapsulates all relevant information accumulated within the LSTM cell up to a specific moment. Simultaneously, the output gate manages which part of this hidden state will serve as the output of the cell at that instant, regulating and filtering the information to be sent to the next layer or another component in the neural network.

18 - ¿En qué consiste el “Teacher Forcing”?

"Teacher Forcing" is a training technique commonly used in sequence generation tasks with recurrent neural networks (RNNs), such as sequence-to-sequence models or language generation models like LSTMs or Transformers.
In "Teacher Forcing," during training, the model is fed with the actual or ground truth target outputs from the training dataset at each time step, rather than using its own generated outputs from the previous time step. Essentially, the model is "forced" to learn from the correct sequence of outputs at each step of the training process.
This approach accelerates learning by providing more accurate and stable gradient signals during training. It helps the model learn the correct sequential relationships and reduces the propagation of errors that might occur due to incorrect predictions in the early stages of training.
However, a potential downside of "Teacher Forcing" is the discrepancy between training and inference modes. During inference, the model might not have access to ground truth outputs and has to rely on its own predictions, which can lead to compounding errors if the model is not adept at handling deviations from the training data.
In summary, "Teacher Forcing" is a training technique that uses actual or ground truth outputs during training to facilitate the learning process in sequence generation tasks for RNN-based models.

19 - ¿Cuál es la diferencia entre atención y auto-atención?

Attention: Attention mechanisms enable neural networks to selectively focus on specific parts or elements of input sequences when making predictions or generating outputs. In models like Seq2Seq or Transformer architectures, attention mechanisms allow the model to weigh different parts of the input sequence differently while processing information. It aggregates information from various parts of the input sequence to generate context-aware representations, enhancing the model's ability to process sequential data.
Self-Attention (also known as Intra-Attention or Internal Attention):Self-attention, commonly found in Transformer-based models, allows elements within the same sequence to attend to each other's representations. It facilitates capturing dependencies between different positions in the input sequence, allowing the model to learn relationships between all elements within the sequence.Self-attention computes representations of each element in the sequence by considering other elements, enabling the model to weigh the importance of each element in relation to others within the same sequence.
In essence, attention mechanisms focus on selective parts of external input sequences, whereas self-attention mechanisms enable elements within the same sequence to attend and establish relationships with each other, aiding in capturing internal dependencies and relationships within the sequence. Both mechanisms enhance the model's ability to understand and generate outputs based on input sequences but operate at different levels of information aggregation and interaction.

20 - ¿Por qué mejoran el proceso de aprendizaje las conexiones residuales?

Residual connections, as featured in Residual Neural Networks (ResNets), notably enhance the learning process in deep neural networks due to specific advantages:
Enhanced Gradient Flow: Residual connections establish direct paths for gradient flow during backpropagation. By circumventing vanishing or exploding gradient issues, these connections facilitate smoother and more effective gradient propagation, especially in very deep networks. This enables better optimization and faster convergence during training.
Deeper Representation Learning: The architecture's ability to learn identity mappings through residual connections allows for the creation of deeper networks. This fosters the exploration of more intricate features and abstract representations within the data. Deeper representations often contribute to improved performance in handling complex datasets, enabling the network to capture more nuanced patterns and variations.

21 - ¿Qué cálculo lleva a cabo la normalización de capa (layer normalization)? 

Layer normalization computes the normalization of activations within a neural network layer. It involves the following steps:
Compute Mean and Variance: For each training sample, calculate the mean and variance across the layer's activations (features).
Normalize Activations: Normalize the activations by subtracting the mean and dividing by the square root of the variance. This step normalizes the activations, ensuring they have a mean of zero and a variance of one.
Scale and Shift: Scale and shift the normalized activations using learnable parameters (gamma and beta). These parameters allow the network to adaptively adjust the normalized values, introducing flexibility for the network to learn the optimal scale and shift for the activations.
The normalization process is applied independently to each individual sample in the batch, providing a form of normalization that aims to stabilize and standardize the activations across different samples and features within a layer of the neural network. This normalization technique aids in improving the convergence and generalization of the network during training.

22 - ¿Por qué es necesario el enmascaramiento durante el proceso de entrenamiento de los Transformers?

Masking during Transformer training is vital for several reasons:

Preventing Future Information Leakage: In language modeling or sequence prediction tasks, it's crucial that the model doesn't access future information during training. Masking ensures that, at each time step, the model only has access to the information available until that point, avoiding the use of future information to predict current elements.
Ensuring Authentic Autonomous Learning: Masking guarantees that the model learns to predict the next word or element in the sequence without relying on future information. This promotes authentic autonomous learning as the model must base its predictions solely on prior data rather than depending on information not available during inference.
Promoting Generalization: By restricting the model's access to future information, masking during training encourages a more generalized representation of data. This assists the model in learning broader patterns and dependencies in sequences, potentially enhancing its ability to generalize to new and unseen data during inference.
In essence, masking during Transformer training ensures that the model doesn't access future information, fostering more authentic learning, preventing leakage of future information, and promoting a more generalized representation of sequences for improved generalization capabilities.

23 - ¿A qué llamamos clasificación “zero-shot”, “one-shot” y “few-shot”?

Zero-shot, one-shot, and few-shot classification refer to different scenarios for training models with limited labeled data:
Zero-shot Classification: In zero-shot classification, a model is trained to classify examples from classes it has never seen during training.The model makes predictions for classes that were not present in the training data by leveraging additional information or semantic relationships learned during training.
One-shot Classification:One-shot classification involves training a model using only a single example per class.The model learns to make predictions for classes it has seen only once during training, requiring it to generalize from very limited examples per class.
Few-shot Classification:Few-shot classification involves training a model with a small number of examples per class, typically more than one but significantly fewer than standard classification tasks. The model learns to generalize from a small dataset, making predictions for classes with limited training examples.
These classification scenarios aim to test the model's ability to generalize and make accurate predictions when provided with limited labeled data or when facing unseen classes during inference. They showcase the model's capacity to learn from sparse or minimal training samples per class and its ability to generalize to new or unseen categories effectively.

24 - ¿Qué es un modelo auto-regresivo?

An autoregressive model is a type of model that predicts future values or sequences based on past observations or inputs. It's a statistical or machine learning model where the output at a given time step depends linearly on its own previous values and possibly on a stochastic term (randomness).
In the context of time series forecasting or sequence generation, autoregressive models predict the next value in a sequence based on previous values in the same sequence. These models assume that the future value of a variable can be expressed as a linear combination of its past values, often employing a fixed number of previous time steps to predict the subsequent value.
Autoregressive models can vary in complexity, from simple linear regression-based models to more sophisticated architectures like Autoregressive Integrated Moving Average (ARIMA) or deep neural networks such as LSTMs (Long Short-Term Memory) and Transformers.
In summary, autoregressive models are characterized by their ability to predict future values or sequences by modeling the relationship between current observations and past values within the same sequence.

25 - Qué son los modelos generativos? 

Generative models are a class of machine learning models designed to generate new data samples that resemble a given dataset. These models learn the underlying patterns and structures present in the training data and use this knowledge to create new, synthetic data points.
There are various types of generative models, each with its own approach to generating data:
Explicit Generative Models: These models learn the probability distribution of the data explicitly. Examples include Variational Autoencoders (VAEs) and Generative Adversarial Networks (GANs). VAEs learn a latent representation of data and generate new samples, while GANs involve a game between a generator and a discriminator to produce realistic samples.
Autoregressive Models: These models predict the probability distribution of the next value in a sequence given the previous values. Autoregressive models, like PixelCNN or Autoregressive Transformers, generate data by predicting one element at a time based on the previous elements.
Flow-Based Models: Flow-based models transform a simple probability distribution (e.g., Gaussian) into a more complex one, allowing them to generate samples resembling the training data. RealNVP and Glow are examples of flow-based models.
Generative models find applications in various fields, such as image generation, text generation, speech synthesis, drug discovery, and data augmentation. They're valuable for creating new synthetic data, generating samples for creative purposes, and enhancing datasets for training other machine learning models.

26 - ¿Cómo es la arquitectura del modelo GPT?

The input to the GPT model consists of tokens representing the text sequence to be processed. These input tokens are passed through an embedding layer that converts each token into a high-dimensional vector representation (embedding) in a continuous space.Since Transformers do not inherently understand the order of tokens, positional encoding is added to the embedded tokens to convey their positions in the sequence.The embedded tokens with positional encodings then pass through multiple layers of Transformer blocks. Each block typically includes self-attention mechanisms that allow tokens to attend to each other's representations in the sequence.Within each Transformer block, residual connections are employed, which allow the output of one layer to bypass another and be added to the output of subsequent layers. This facilitates smoother gradient flow and helps in avoiding vanishing gradients in deep networks.Layer normalization is applied after each sub-layer (such as encoder-decoder-attention and feedforward layers) in the Transformer block. This normalizes the output of each sub-layer, aiding in stabilizing the training process.Following the encoder-decoder mechanism and normalization, a feedforward neural network with activation functions (such as ReLU) processes the data, capturing complex patterns and interactions within the sequence.Finally, the processed tokens from the last layer of the Transformer architecture are linearly transformed and passed through a softmax function to generate a probability distribution over the vocabulary. The token with the highest probability is often chosen as the predicted token or the next word in the sequence.
The last predicted token is typically used to generate subsequent tokens in a sequence, effectively initiating the generation process. This token output often becomes the input for the next time step in autoregressive language generation tasks.

27 - ¿Cómo es la arquitectura del modelo BERT?

BERT, known as Bidirectional Encoder Representations from Transformers, features a Transformer-based architecture exclusively built on the encoder side. It operates on tokenized input sequences, converting them into fixed-size embeddings using WordPiece tokenization, allowing for better representation of rare words and complex languages.
Using a series of stacked Transformer encoder blocks, BERT captures bidirectional context by incorporating positional encodings to comprehend the sequence order. Its key innovation lies in unsupervised pre-training, where it employs Masked Language Modeling (MLM) and Next Sentence Prediction (NSP) objectives. MLM trains the model to predict masked tokens within the input sequence, while NSP focuses on predicting if two sentences are consecutive in the original text.
After pre-training on extensive text corpora, BERT can be fine-tuned for specific tasks by adding task-specific layers and adjusting model parameters using smaller, task-specific datasets. This architecture enables BERT to capture intricate contextual information bidirectionally, leading to remarkable performance in various natural language processing tasks.

28 - Hugging Face

Hugging Face is a company and an open-source community that focuses on Natural Language Processing (NLP) and provides tools, libraries, and frameworks to facilitate NLP development. Their primary offerings include:
Transformers Library: Hugging Face developed the Transformers library, a popular open-source library that provides an easy interface to access pre-trained language models like BERT, GPT, RoBERTa, and more. It offers pre-trained models, fine-tuning utilities, and functionalities for various NLP tasks.
Datasets Library: Their Datasets library is another open-source tool offering an extensive collection of datasets for NLP tasks. It simplifies the process of accessing and processing various datasets, making them readily available for research and development purposes.
Tokenizers: Hugging Face offers tokenization tools and libraries that efficiently preprocess text data, segmenting it into tokens suitable for NLP models' input.
Model Hub: They maintain a Model Hub, a platform where users can access, share, and use pre-trained models, allowing researchers and developers to explore, fine-tune, and leverage state-of-the-art models for their projects.
Overall, Hugging Face is renowned for its contributions to the NLP community, offering essential tools, libraries, pre-trained models, datasets, and collaborative platforms that significantly simplify NLP research, development, and implementation.

29 - ¿Por qué se divide el resultado del producto escalar de los vectores Q y K por la raíz cuadrada de la dimensión de K?

Dividing the dot product of vectors Q and K by the square root of the dimension of K in the attention mechanism serves to normalize the resulting values. This process, known as attention scaling, controls the scale of attention scores. It prevents values from becoming too large or too small, ensuring stability during training and aiding the model in learning meaningful patterns in the data. and not depending from the measure of k

30 - ¿Por qué los Transformers usan múltiples cabezas de atención?


Transformers utilize multiple attention heads to process information simultaneously and capture diverse patterns in input sequences. This approach allows the model to focus on different aspects of the input in parallel, enabling the capture of complex relationships and varied levels of abstraction. With each head attending to distinct parts of the data, the model can learn richer, more detailed representations, enhancing its understanding of specific features relevant to the task at hand. Additionally, employing multiple attention heads introduces implicit regularization, reducing overfitting by encouraging the model to consider diverse perspectives during training. Overall, these multiple heads enhance the model's capacity to comprehend intricate patterns, provide richer representations, and offer varied perspectives for more effective learning.
